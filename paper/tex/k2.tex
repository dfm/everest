\documentclass[]{emulateapj}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath,mathtools}
\usepackage{epsfig}
\usepackage[FIGTOPCAP]{subfigure}
\usepackage{afterpage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{relsize}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{morefloats}
\usepackage{wasysym}

\newcommand{\noop}[1]{}
\newcommand{\note}[1]{{\color{red} #1}}
\newcommand{\cn}{\note{(citation needed)\ }}
\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}
\newcommand{\mearth}{\unit{M_\oplus}}
\newcommand{\rearth}{\unit{R_\oplus}}
\newcommand{\msun}{\unit{M_\odot}}
\newcommand{\lsun}{\unit{L_\odot}}
\newcommand{\mstar}{\unit{M_\star}}
\newcommand{\rj}{\ensuremath{R_\mathrm{J}}}
\newcommand{\avg}[1]{\langle #1 \rangle}
\newcommand{\bavg}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}

\shorttitle{EVEREST}
\shortauthors{Luger et al. 2016}

\begin{document}

\title{EVEREST: Pixel Level Decorrelation of \emph{K2} Light curves}

\author{R. Luger\altaffilmark{1,2}, E. Agol\altaffilmark{1,2}, R. Barnes\altaffilmark{1,2}, and E. Kruse\altaffilmark{1}}
\affil{\altaffilmark{1}Astronomy Department, University of Washington, Box 351580, Seattle, WA 98195, USA; rodluger@uw.edu\\
\altaffilmark{2}Virtual Planetary Laboratory, Seattle, WA 98195, USA}

\begin{abstract}
We present \texttt{EVEREST} (\emph{EPIC Variability Extraction and Removal for
Exoplanet Science Targets}), an open-source pipeline for removing instrumental noise
from \emph{K2} light curves. \texttt{EVEREST} employs a variant of the pixel-level decorrelation (PLD)
technique developed by \cite{DEM15} to remove systematics introduced by the spacecraft's 
pointing error and a Gaussian process (GP) to capture astrophysical variability. We
apply \texttt{EVEREST} to all \emph{K2} targets in Campaigns 0-6, yielding light curves
with precision comparable to that of the original \emph{Kepler} mission for stars brighter
than $K_p \approx 13$, and within a factor of two of the \emph{Kepler} precision for fainter
targets. We further compare our light curves to the other de-trended light curves \note{available
at MAST HLSP} and demonstrate that, on average, \texttt{EVEREST} results in the highest precision of any
pipeline for $K_p \lesssim 15$ stars. The \texttt{EVEREST} catalog and the source code
used to generate it are freely available online.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Launched in 2009, the \emph{Kepler} spacecraft has to date led to the discovery
of nearly 5000 extrasolar planet candidates
and to a revolution in several fields of astronomy including but not limited to
exoplanet science, eclipsing binary characterization, asteroseismology and stellar variability studies.
Its unprecedented photometric precision allowed for the study of astrophysical signals
down to the level of $\sim 15$ parts per million \citep{GIL11}, which enabled the discovery
of small planets in the habitable zones of their stars \citep[e.g.,][]{BOR13,QUI14,TOR15}.
Unfortunately, after the failure of its second reaction wheel in May 2013, the spacecraft was
no longer able to achieve the fine pointing accuracy required for high precision photometry,
and the nominal mission came to an end. Engineering tests suggested that by aligning
the spacecraft along the plane of the ecliptic, pointing drift could be mitigated
by the solar wind pressure and by periodic thruster firings. As of May 2014 the spacecraft has been operating in
this new mode, known as \emph{K2}, and has continued to enable high precision photometry
science, monitoring tens of thousands of stars near the plane of the ecliptic during campaigns 
lasting about 75 days each.

However, because of the reduced pointing accuracy, \emph{K2} raw photometry is between 
3 and 4 times less precise than that of the original \emph{Kepler} mission and displays
strong instrumental artefacts on $\sim$ 6 hour timescales, which
severely compromise its ability to detect small transits. Recently, several authors
have developed powerful methods to correct for these systematics, often coming within
a factor of $\sim$ 2 of the original \emph{Kepler} precision.

In particular, the \texttt{K2SFF} pipeline \citep{VJ14} decorrelates \emph{K2} aperture 
photometry with the centroid position
of the stellar images. Centroids are determined based either on the center-of-light
or via a Gaussian fit to the stellar PSF. The motion of the centroids is then fit with a polynomial
and transformed into a single parameter that relates spacecraft motion to flux variations,
which is then used to de-trend the data. Similar methods are employed in the \texttt{K2VARCAT} pipeline \citep{ARM15}, 
developed specifically for variable \emph{K2} stars, the \texttt{K2P$^2$} pipeline \citep{LUN15},
which uses an intelligent clustering algorithm to define custom apertures, and in the pipeline
of \cite{HUA15}, which employs astrometric solutions to the motion of \emph{K2} targets,
determining the $X$ and $Y$ motion of a target from the behavior of multiple stars on
the same spacecraft module. Finally, the \texttt{K2SC} pipeline \citep{AIG15,AIG16} and 
the pipeline of \cite{CRO15} both employ a Gaussian process (GP) to remove
instrumental noise, using the $X$ and $Y$ coordinates of the target star as the regressors to derive
a model for the instrumental systematics. The nonparametric nature of the GP results 
in a flexible model with increased de-trending power especially for dim \emph{K2}
targets.

In one way or another, all of these methods rely on numerical methods to identify and 
remove correlations between the stellar position and the intensity fluctuations. Even
when a nonparametric technique such as a GP is used, assumptions are still made about
the nature of the correlations between spacecraft motion and instrumental variability.
Moreover, the process of determining the image centroids is prone to uncertainties
and relies on assumptions about the shape of the stellar PSF.

A powerful alternative to these methods is pixel-level decorrelation (PLD), a method developed
by \cite{DEM15} to correct for systematics in \emph{Spitzer} observations of transiting
hot Jupiters. The tenet of PLD is that the \emph{best way to correct for noise introduced by
the motion of the stellar image does not involve actual measurements of the position
of the star}. The position of the stellar image is, after all, a secondary data product
of photometry, and is subject to additional uncertainty.
PLD skips these two numerical steps (i.e., fitting for the stellar position and solving
for the correlations) by operating on the \emph{primary} data products of photometry, the
intensities in each of the detector pixels. These intensities are normalized by the
total flux in the chosen aperture then used as basis vectors for an ordinary least-squares 
(OLS) fit to the aperture-summed flux. Since astrophysical signals (stellar variability,
planet transits, stellar eclipses, etc.) are present in all of the pixels in the aperture,
the normalization step removes astrophysical information from the basis set, ensuring
that PLD remove only the signals that are \emph{different} across the aperture. PLD is
therefore an ``agnostic'' method of performing robust flat-fielding corrections with
minimal assumptions about either the nature of the intra-pixel variability or the correlation 
between spacecraft jitter and intensity fluctuations. We note that our method is similar 
to that of \cite{DFM15} and \cite{MON15}, who use the principal components of the variability 
among the full set of \emph{K2} Campaign 1 light curves as ``eigen light curve'' regressors.
However, rather than deriving our basis vectors from other stars in the field, whose
light curves contain undesired astrophysical signals, our basis vectors are derived
solely from the pixels of the star under consideration.

In this paper we build on the PLD method of \cite{DEM15}, extending it to higher order
in the pixel fluxes and performing principal component analysis (PCA) on the basis
vectors to generate the ideal set of regressors. We further couple PLD with a GP to 
disentangle astrophysical and instrumental variability. We apply our pipeline, \texttt{EVEREST}
(\emph{EPIC Variability Extraction and Removal for
Exoplanet Science Targets}), to the entire set of \emph{K2} light curves from Campaigns 0-6 and generate a 
publicly-available database\footnote{\texttt{\note{\url{https://archive.stsci.edu/prepds/everest/}}}}
of processed light curves. Our code is open-source and
available online.\footnote{\texttt{\url{https://github.com/rodluger/everest}}}

The paper is organized as follows: in \S\ref{sec:pld} we review the basics of PLD and
derive our third-order PLD/PCA/GP model, and in \S\ref{sec:methods} we describe our pipeline
in detail. Results are presented in \S\ref{sec:results}, and in \S\ref{sec:conclusions}
we conclude and outline plans for future work.

\section{Pixel-Level Decorrelation}
\label{sec:pld}
\subsection{First Order PLD}
\label{sec:firstorder}
The linear PLD model developed in \cite{DEM15} is given by the expression
\begin{align}
\label{eq:demingmodel}
m_i = &\sum\limits_{l}a_l\frac{p_{il}}{\sum\limits_{k}p_{ik}} + \alpha + \beta t_i + \gamma t_i^2
\end{align}
where $m_i$ denotes the noise model at time $t_i$,
$p_{il}$ denotes the flux in the $l^{th}$ pixel at time $t_i$, and $a_l$ is
the linear PLD coefficient for the $l^{th}$ pixel. Both sums are taken over
all pixels in the aperture; the last three terms are a polynomial in time used
to capture temporal variations in the flux baseline.

The coefficients are obtained by minimizing the sum of the squares of the difference
between the model $m_i$ and the transit-corrected simple aperture photometry (SAP) flux, $y_i$:
\begin{align}
\label{eq:demingsolution}
\frac{\partial \chi^2}{\partial a_l} = 0
\end{align}
where
\begin{align}
\label{eq:demingchisq}
\chi^2 = \mathlarger{\mathlarger{\sum\limits_{i}}}\frac{\left(y_i - m_i\right)^2}{\sigma_i^2}
\end{align}
where $\sigma_i$ are the standard errors of the flux and
\begin{align}
\label{eq:demingcorrsap}
y_i = \frac{1}{\tau_i}\sum\limits_{k}p_{ik}.
\end{align}
Division by the transit model $\tau_i$ is necessary because $m_i$ contains no transit
information, since it is a function only of the fractional pixel fluxes, which
contain no transit signal themselves.

Framed in this manner, computation of the PLD model is a linear regression problem; the
coefficients $a_l$ are readily found by solving Equation~(\ref{eq:demingsolution}).
However, the transit model $\tau_i$ is generally unknown and must also be obtained from
the data, in which case the linearity is lost. \cite{DEM15} thus ran MCMC chains to
simultaneously solve for the PLD coefficients and the parameters of the transit model.

Alternatively, one may instead solve Equation~(\ref{eq:demingsolution}) using the
out-of-transit flux only (i.e., by masking all arrays during transit). 
Since $\tau_i = 1$ outside of transit, the problem remains linear, allowing one to
easily obtain the model $m_i$ conditioned on the out-of-transit flux. In the limit that
the transit duration is short relative to the orbital period (which is the case for most
exoplanets), we find that there is no practical difference between the two methods.

\subsection{Higher Order PLD}
\label{sec:higherorder}
\begin{figure}[h]
  \begin{center}
      \psfig{file=images/third_order.png,width=3in}
       \caption{PLD applied to a portion of the data for EPIC 201367065 (K2-3). 
       The top panel is the background-subtracted, normalized SAP flux in a large 
       35-pixel aperture centered on the target. The bottom three panels show 
       the normalized PLD-de-trended flux for 1$^{\mathrm{st}}$, 2$^{\mathrm{nd}}$, and 
       3$^{\mathrm{rd}}$ order PLD, respectively, using only the 10 brightest pixels. 
       PLD decreases the 6-hr photometric precision by factors of 2.9 (1$^{\mathrm{st}}$ order), 4.7 
       (2$^{\mathrm{nd}}$ order), and 5.2 (3$^{\mathrm{rd}}$ order).\\[0.25in]}
     \label{fig:third_order}
  \end{center}
\end{figure}

\cite{DEM15} found that the pointing jitter of the \emph{Spitzer} telescope was 
sufficiently small that extending PLD to higher order in the pixel fluxes was
unnecessary. However, because of the large pointing variations of the \emph{K2}
spacecraft on short timescales, we find it necessary to extend PLD to higher order.
Keeping terms up to third order in the pixel fluxes, we may express this model as 
\begin{align}
\label{eq:pldmodel}
m_i = &\sum\limits_{l}a_l\frac{p_{il}}{\sum\limits_{k}p_{ik}} + \nonumber\\
      &\sum\limits_{l}\sum\limits_{m}b_{lm}\frac{p_{il}p_{im}}{(\sum\limits_{k}p_{ik})^2} + \nonumber\\
      &\sum\limits_{l}\sum\limits_{m}\sum\limits_{n}c_{lmn}\frac{p_{il}p_{im}p_{in}}{(\sum\limits_{k}p_{ik})^3} + \alpha + \beta t_i + \gamma t_i^2
\end{align}
where once again index $i$ denotes time and all other indices denote the pixel number. 
The PLD coefficients are now $a_l$ (one per pixel), 
$b_{lm}$ (one per pixel pair), and $c_{lmn}$ (one per group of three pixels). Despite
the added complexity, the model remains linear and may be solved in a similar fashion 
as before.

In Figure~\ref{fig:third_order}, we illustrate the de-trending technique for EPIC
201367065 (K2-3), a star host to at least two transiting planets \citep{CRO15}. The top
panel shows the normalized raw aperture-summed flux after subtracting off the 
background; note the large systematics at very short ($\sim$ 6 hr) timescales. The
next panel shows the flux de-trended with first order PLD (Equation~\ref{eq:demingmodel}),
having masked the known transits. The scatter is reduced by a factor of 2.9 and the
seven transits become visible by eye. The following two panels show the results of
second and third order PLD, which improve the scatter over the raw data by factors of 
4.7 and 5.2, respectively. Note, importantly, that even data collected during thruster
fire events (the outlier points seen above the continuum every $\sim$ 6 hr in the top 
two plots), is properly corrected by higher order PLD.

One might wonder why not go to even higher order PLD. While possible in principle,
one must take care not to overfit, given the steep increase in the number of
coefficients as one goes to higher order. Given that $K2$ apertures must be relatively
large to properly enclose all of the flux of the (moving) source, the number of
coefficients for third order PLD can easily reach several thousand. For a star with an
aperture of 30 pixels (typical of a $Kepler$ magnitude $\sim$ 13 star), the number of
coefficients in Equation~(\ref{eq:pldmodel}) is
\begin{align}
n_{pld} &= \mathlarger{\mathlarger{\sum\limits_{k=1}^{3}}}{n_{pix} + k - 1 \choose k} \nonumber\\
        &= 30 + 465 + 4960 \nonumber\\
        &= 5455.
\end{align}
Given an average of $\sim$ 3750 long cadence data points in a typical \emph{K2} 
campaign, third order PLD would already grossly overfit most \emph{K2} targets. To
avoid this, one option is to perform PLD on a subset of the pixels in the aperture,
preferably the brightest ones. This is what we did in Figure~\ref{fig:third_order}, 
where for 10 pixels the number of coefficients is 285.

However, rather than assume that the brightest $N$ pixels contain the most
de-trending power, a more agnostic (and better) approach is to compute the principal components of the
variability directly. We perform principal component analysis (PCA) on the first,
second, and third order fractional pixel fluxes (the terms in Equation~\ref{eq:pldmodel})
to obtain a set of $N$ basis vectors $\mathbf{x}$ that describe most of the instrumental
variability in the light curve. We use these to construct our \emph{design matrix} 
\begin{align}
\label{eq:designmatrix}
\mathbf{X} =  \begin{pmatrix}
              x_{0,0} & x_{0,1} & ... & x_{0,N} & 1\\
              \\
              x_{1,0} & x_{1,1} & ... & x_{1,N} & 1\\
              \\
              ... & ... & ... & ... & ...\\
              \\
              x_{M,0} & x_{M,1} & ... & x_{M,N} & 1\\
              \end{pmatrix}
\end{align}
where $M$ denotes the total number of data points along the time dimension. We choose
the number of principal components $N$ that maximize the de-trending power while preventing
overfitting. We explain our method in \S\ref{sec:dmopt} below; typically, $N < 100$.

Since our problem is still linear, our model is simply
\begin{align}
\label{eq:pcamodel}
\mathbf{m} = \mathbf{X}^\top \cdot \mathbf{c}
\end{align}
where $\mathbf{c}$ is the vector of coefficients, one for each basis vector in $\mathbf{X}$.
Their values are given by solving the generalized least-squares (GLS) problem
\begin{align}
\label{eq:pcacoefficients}
\mathbf{c} = \left(\mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X}\right)^{-1} \cdot \left(\mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}\right)
\end{align}
where $\mathbf{K}$ is the covariance matrix of the data and $\mathbf{y}$ is the transit-corrected
SAP flux given by Equation~(\ref{eq:demingcorrsap}). Note that for a diagonal covariance
$\mathbf{K}_{ij} = \delta_{ij}\sigma_i^2$, Equation~(\ref{eq:pcacoefficients}) is
mathematically equivalent to Equations~(\ref{eq:demingsolution}) and 
(\ref{eq:demingchisq}) from before.

\subsection{Gaussian Processes}
\label{sec:gp}
\begin{figure*}[t]
  \begin{center}
    \leavevmode
      \psfig{file=images/sinusoid.png,width=\textwidth}
       \caption{Different de-trending techniques for quarter 4 of KIC 8583696 (KOI 1275),
                a planet candidate host from the original \emph{Kepler} mission. The original
                data is shown in the left column; in the other columns we
                artificially injected a sinusoidal signal with a period
                of 25 days and an amplitude comparable to that of the instrumental variability.
                The top row shows the raw SAP data (black) and the first order
                PLD model (red); the residuals of the fit are indicated directly below. 
                The third row shows the final residuals after smoothing 
                with a GP to eliminate low-frequency stellar variability. Finally, the bottom
                row shows these residuals folded on the orbital period of the planet
                candidate (black), with the 5-hr median indicated in red. Combining PLD with a GP
                ensures PLD fits out only the instrumental variability without inflating the white noise.
                }
     \label{fig:sinusoid}
  \end{center}
\end{figure*}

In Equation~(\ref{eq:designmatrix}) we purposefully neglected the temporal polynomial
terms. In principle, modeling stellar variability should not be necessary if all we wish 
to remove is the instrumental noise. By virtue of using the
\emph{fractional} pixel fluxes as basis vectors (from which the astrophysical signal
has been removed by the normalization), PLD should fit out instrumental variability only,
obviating the need for an extra temporal term. However, in practice, this is not
the case. To illustrate this, in Figure~\ref{fig:sinusoid} we plot a quarter of data
of KOI 1275, a star with one planet candidate discovered by the original \emph{Kepler}
mission \citep{BOR11}. We chose this star over one from the \emph{K2}
mission simply because it is easier to discriminate between instrumental and
astrophysical signals by eye for this light curve, though our arguments apply
equally well to \emph{K2}.

Figure~\ref{fig:sinusoid}\textbf{a} shows the raw simple aperture photometry (SAP) data in black (top
panel); in red we plot a simple first-order PLD fit with no polynomial term (obtained
from the solution to Equation~\ref{eq:demingsolution}). Below it are 
the residuals of the PLD fit (second panel, black) and the fully de-trended data after 
smoothing with a GP to remove stellar noise (third panel). The bottom panel shows 
the de-trended data folded on the period of the planet candidate, with the 5-hr 
median shown in red. Note that the PLD fit is quite good, as it removes the
low-frequency arc as well as the flux jump at $t \sim 375$ d and the thermal 
features at $t \sim 385$ d and $t \sim 400$ d. Since the star
isn't significantly variable, the temporal term is not necessary.

However, this is not the case when the star is variable. In the next three columns 
we multiply the pixel-level light curve by a sinusoid 
with a period of 25 days to simulate stellar variability. We choose an amplitude 
comparable to the amplitude of the instrumental signal, and de-trend the light 
curve three different ways: PLD only (\textbf{b}), PLD plus a tenth order 
polynomial (\textbf{c}) and PLD plus a GP (\textbf{d}).

The PLD-only fit (Figure~\ref{fig:sinusoid}\textbf{b}) is quite poor. The general shape of the PLD
curve is mostly preserved and, as expected, the stellar signal is still present 
in the residuals, but the fit increases the white noise by a factor of $\sim$ 4,
all but washing out the transits (bottom panel). This happens because of a
serious limitation of the OLS problem framed in Equation~\ref{eq:demingsolution}, 
which yields the PLD coefficients that minimize the scatter of the PLD term $m_i$ 
about the flux $y_i$. If $m_i$ is not a suitable approximation to the flux, as in the case
of a highly variable star, the $\chi^2$ of the fit will necessarily be large.
It is not hard to show that $\chi^2$ can be substantially reduced
by inflating the white noise component of $m_i$, leading to the large scatter
seen in the de-trended data. Absent a model for the astrophysical variability,
PLD naturally exchanges correlated noise for white noise, which severely
compromises its de-trending power. 

A straightforward way to improve the quality of the fit is to include the polynomial
term to capture the stellar variability, as in \cite{DEM15}. We do this in Figure~\ref{fig:sinusoid}\textbf{c}, 
where we use a tenth order polynomial. While the fit is significantly
improved, PLD still inflates the white noise component, since the polynomial is 
not flexible enough to capture all of the stellar signal. Though the transit is
visible in the bottom plot, the quality of the de-trended data is significantly
degraded when compared to that of the non-variable star.

One might imagine that a higher order polynomial would do the trick. However, there
is an alternate approach, one that naturally follows from Equation~\ref{eq:pcacoefficients}.
Rather than model the stellar signal explicitly, we instead treat it non-parametrically
with a GP, which we use to estimate the covariance matrix $\mathbf{K}$. Provided
$\mathbf{K}$ is a reasonable approximation to the true covariance of the stellar signal,
Equation~\ref{eq:pcacoefficients} will yield a set of PLD coefficients that 
fit out only the instrumental component of the noise. We illustrate this in 
Figure~\ref{fig:sinusoid}\textbf{d}, where we use a Mat\'ern-3/2 kernel 
(see Equation~\ref{eq:kernels} below) with amplitude $\alpha_m = 100$ and timescale
$\tau_m = 20$ days to model the stellar signal. Unlike in the previous two cases,
the PLD model (top panel, red) no longer attempts to fit out the stellar signal; in 
fact, the curve is almost identical to the fit to the original data 
(Figure~\ref{fig:sinusoid}\textbf{a}). The astrophysical signal is
recovered to high fidelity by subtracting the PLD term (second panel). After removing it,
the final residuals (third panel) show a comparable (in fact, marginally smaller) RMS to the
residuals of the original data. It is also clear from the bottom panel that the transit 
shape and depth are fully preserved.

We note, finally, that another advantage of the GP approach is that the PLD model is
relatively insensitive to the particular choice of kernel function and values of its
hyperparameters. This is evident from the example above; even though the stellar
variability signal was generated from a 25-day period sinusoidal function, a radial
Mat\'ern-3/2 kernel with a timescale of 20 days was sufficient to ensure the instrumental
signal was removed without inflating the white noise. This is important because the
stellar signal is generally unknown \emph{a priori}, and any attempt to estimate it from the data
will be subject to how well one can discriminate between instrumental and stellar effects.
In \S\ref{sec:methods} we describe our iterative procedure for optimizing the GP kernel.

\section{Methods}
\label{sec:methods}
In \S\ref{sec:pld} we outlined the basics of principal component regression
on the fractional pixel fluxes using a GP. In this section, we describe how we apply
this method to \emph{K2} data.

\subsection{Pre-Processing}
\label{sec:pre}
For each star in the \emph{K2} EPIC catalog, we download the target pixel files and
select aperture number 15 from the \texttt{K2SFF} catalog \citep{VJ14, VAN14}, which is derived
by fitting the Kepler pixel response function (PRF) to the stellar image. The \texttt{K2SFF}
data contain twenty such apertures of varying sizes for each star. We find that aperture 15
is typically the best compromise between having enough pixels to generate a good basis
set for PLD while preventing excess contamination from neighboring stars. We note that
when contamination is not an issue, the choice of aperture has little effect on our results.
As in \cite{VJ14}, we then compute the median per-timestamp flux in the pixels of the image 
that lie outside the aperture and subtract it from all pixels
to remove the background signal. In this paper, we consider only the long cadence (LC)
$\emph{K2}$ dataset.

Next, we mask any known transits, eclipses, or astrophysical false positives, as neither
PLD nor the GP are able to accurately model these, which could lead to white noise
inflation as described in \S\ref{sec:gp}. These are obtained from the Exoplanet Archive
K2 Candidates database\footnote{\texttt{\url{http://exoplanetarchive.ipac.caltech.edu}}} and from
the Kepler Eclipsing Binary Catalog \citep{LAC15}. Note that these data points are masked solely for
the purpose of computing the PLD coefficients, and we include them in the final
de-trended datasets. Similarly, we find it necessary to mask large outliers before
performing PLD. We apply a 5-cadence median filter to the SAP flux and subtract it
from the flux, then perform a median absolute deviation (MAD) cut at 5 sigma to 
identify potential outliers. For each outlier identified this way, we then compute
a first order PLD model in the vicinity of the outlier, with and without the outlier
included in the data. If the RMS of the de-trended data decreases when the outlier
is removed, we mask it during our analysis; otherwise we keep it in the light curve.

\subsection{GP Optimization}
\label{sec:gpopt}
\begin{figure*}[t]
  \begin{center}
    \leavevmode
      \psfig{file=images/acor.png,width=\textwidth}
       \caption{GP optimization procedure for EPIC 201497682. In the top left panel we 
                plot the raw SAP flux (black) and a ten chunk, first order PLD fit
                (red); the residuals are shown in the panel below. These are used to
                compute the power spectrum of the stellar signal (top right), and
                its autocorrelation function (bottom right, black curve). Different
                kernels are then fit to the autocorrelation function, and the one
                with the lowest $\chi^2$ value is chosen for the de-trending step
                (blue curve). The grey envelope about the autocorrelation curve is
                the ad hoc standard error assumed to compute $\chi^2$.
                }
     \label{fig:acor}
  \end{center}
\end{figure*}

The next step is to compute the covariance matrix $\mathbf{K}$, which we do using
the \texttt{george}\footnote{\texttt{\url{http://dan.iel.fm/george/current/}}} Python package. We parametrize 
it as
\begin{align}
\label{eq:covariance}
\mathbf{K}_{ij} = k_w(t_i, t_j) + k_t(t_i, t_j)
\end{align}
where
\begin{align}
\label{eq:whitekernel}
k_w(t_i, t_j) = \sigma^2\delta_{ij}
\end{align}
is a white noise kernel with standard deviation $\sigma$ and $k_t$ is either an 
additive or multiplicative combination
of one or more of an exponential kernel $k_e$, a Mat\'ern-3/2 kernel $k_m$, and
a cosine kernel $k_c$:
\begin{align}
\label{eq:kernels}
k_e(t_i, t_j) &= \alpha_e e^{-\left|t_i - t_j\right|/\tau_e}\nonumber\\
k_m(t_i, t_j) &= \alpha_m \left(1 + \sqrt{3(t_i - t_j)^2}\right) e^{-\sqrt{3(t_i - t_j)^2}/\tau_m}\nonumber\\ 
k_c(t_i, t_j) &= \alpha_c \cos{\left(\frac{2\pi}{P}(t_i - t_j)\right)}.
\end{align}
The choice of kernel depends on the properties of the stellar signal, which we do
not know \emph{a priori}, since it is mixed with the instrumental signal in the
light curve. We therefore adopt an iterative procedure, where we guess at the initial
kernel form and hyperparameters and use it to de-trend the light curve with PLD, thus
obtaining an approximate stellar component. We then train the GP on this stellar
component and use it to run our PLD analysis again. While this can be repeated
multiple times, we find that once is typically enough for most targets. This procedure
is illustrated in Figure~\ref{fig:acor}.

Our initial kernel is a Mat\'ern-3/2 kernel with $\tau_m = 2$ days and amplitude
equal to the median variance in 2-day chunks of the light curve. We split the light curve
into ten roughly equal chunks and de-trend each of them with first order PLD
using Equations~\ref{eq:pcamodel} and \ref{eq:pcacoefficients}. We then compute the 
autocorrelation function of the de-trended data and perform least-squares fits
on different combinations of the kernels in Equation~\ref{eq:kernels}, using
the peaks in a Lomb-Scargle periodogram as the initial
guess for the periods in the cosine kernels. We choose the kernel and corresponding
hyperparameters that result in the best fit and compute its overall amplitude
as well as the amplitude of the white noise kernel $k_w$
by maximizing the marginal likelihood $\mathcal{L}$ of the data given the model,
\begin{align}
\label{eq:like}
\log\mathcal{L} = \frac{1}{2}\mathbf{y}^\top\cdot\mathbf{K^{-1}}\cdot\mathbf{y} - \frac{1}{2}\log\left|\mathbf{K}\right| - \frac{n}{2}\log 2\pi
\end{align}
where $\mathbf{y}$ is the de-trended flux and $n$ is the number of data points 
in $\mathbf{y}$ \citep{RW06}. In principle, one may optimize the parameters of each
of the kernel combinations in this way to obtain the best estimate of the covariance
matrix. However, such a procedure is computationally expensive. After considerable
experimentation, we find that the method outlined above---which takes less than one
minute on a single core of a 2.66 GHz machine---is sufficient to ensure PLD removes
only the instrumental component of the noise for most targets.

\subsection{Design Matrix Optimization}
\label{sec:dmopt}
The next step in our pipeline is to construct our design matrix 
(Equation~\ref{eq:designmatrix}). In order to do so, we must choose the
number of PLD principal components to use in the regression. This number must be
large enough to capture most of the instrumental variability
but not so large as to lead to overfitting. In fact, one must take care
to prevent PLD (or any other de-trending technique) from fitting out the white noise 
component of the light curve. As the number of basis vectors grows to become very large, 
there begin to exist linear combinations of these vectors that can artificially remove 
white noise from the data, improving the apparent quality of the fit but leading to spurious 
results. This is best illustrated by considering the example in Figure~\ref{fig:third_order}, 
where we kept only 285 basis vectors in the bottom panel. Had we de-trended with all 
8435 components, we would obtain the light curve shown in Figure~\ref{fig:overfitting}.
\begin{figure}[h]
  \begin{center}
      \psfig{file=images/overfitting.png,width=3.5in}
       \caption{\emph{Top}: Third order PLD applied to EPIC 201367065, but this time keeping 
                \emph{all} basis vectors. Compare to 
                Figure~\ref{fig:third_order}. While the median scatter improved by a factor
                of about 2, the scatter in the transits (which were masked during the de-trending)
                increased by a factor of several thousand. \emph{Bottom}: The same figure, 
                but zoomed out to show the in-transit scatter.}
     \label{fig:overfitting}
  \end{center}
\end{figure}
While the median scatter improved by a factor of $\sim$~2, the in-transit scatter increased
by a factor of several thousand. This is because we masked the transits of \emph{K2-3b},
\emph{c}, and \emph{d} during the de-trending step (see \S\ref{sec:firstorder}). The poor
performance of the extrapolated model betrays its terrible predictive power, a clear sign
of overfitting.

One way to prevent this is to use an information criterion, such as the Bayesian Information 
Criterion (BIC), which adds a penalty term to the likelihood (Equation~\ref{eq:like}) 
that grows in size with the number of model parameters \citep[see, e.g.,][]{DFM15}.
However, in the spirit of the previous example, we find that a more agnostic and data-driven
approach is to measure the amount of overfitting by \emph{directly} computing the predictive power
of the model in masked regions of the data. To this end, for a range of design matrix sizes, we 
compare the RMS in 6-hr chunks of the de-trended data to the RMS in 6-hr chunks that we purposefully 
masked during the de-trending step. This process is illustrated in Figure~\ref{fig:scatter}.

\begin{figure}[h]
  \begin{center}
      \psfig{file=images/scatter.png,width=3.5in}
       \caption{De-trended light curve precision as a function of the number of principal
                components for EPIC 201497682. The blue curve is the median
                6-hr RMS scatter of the entire light curve; the red curve is the median 
                scatter in 6-hr chunks that were masked during the de-trending step.
                Initially, the scatter decreases in both cases as the 
                number of components is increased. However, above $\sim$ 50 components, while the apparent
                scatter continues to decrease, the scatter in the masked regions (where 
                the model is extrapolated) begins to grow. This is the signature
                of overfitting. We therefore choose 50 principal components for the
                de-trending, yielding a precision of 55 ppm (versus $\sim$ 70 ppm for the
                \texttt{K2SFF} de-trended flux).}
     \label{fig:scatter}
  \end{center}
\end{figure}

Initially, as the number of principal components $n_{pc}$ increases, the scatter in the de-trended flux
decreases in the full light curve (blue curve) as well as in the masked portions of the light curve
(red curve). As the number of components increases further, the nominal precision continues to improve,
eventually surpassing the photon noise limit for $n_{pc} \gtrsim 1000$. This is obviously unphysical and a
clear sign of overfitting. The red curve, on the other hand, begins to increase steadily above $n \sim 50$,
indicating that the predictive power of the model worsens as the number of principal components is increased.
The minimum in the red curve, which occurs for $n_{pc} \sim 50$, is the best we can do; above that point, PLD
is likely to begin fitting out white noise.

We employ this bootstrapping method for all EPIC targets. In practice, we compute the scatter in masked
and unmasked chunks of the light curve thirty times for each value of $n_{pc}$ and take the median. Our
grid in $n_{pc}$ typically ranges from 1 to 250 principal components, with a spacing of ten components.
We then smooth the curves with a GP and find the value of $n_{pc}$ that minimizes \emph{both} the masked scatter
\emph{and} the absolute value of the difference between the masked and unmasked scatter.

In practice, we also find that varying the PLD order can also lead to better fits, and thus we perform the
procedure outlined above for first, second, and third order PLD. Finally, similarly to \cite{AIG16}, 
we occasionally find it necessary to split a light curve into two or more separate chunks. Reversals in
the direction of the spacecraft roll during a campaign can qualitatively change the instrumental
systematics. This is evident in the top left panel of Figure~\ref{fig:acor}, where the systematics
become much more apparent after $t \sim 2025$ d. Like \cite{AIG16}, we identify these breakpoints
by visual inspection, but note that the exact location where we split the light curve does not
significantly change our results. This division is easily implemented by partitioning the design
matrix $\mathbf{X}$ as follows:
\begin{align}
\label{eq:designmatrixsplit}
\mathbf{X'} =  
\begin{pmatrix}
x_{0,0} & ... & x_{0,N} & 1 & 0 & ... & 0 & 0\\
\\
... & ... & ... & ... & ... & ... & ... & ...\\
\\
x_{B,0} & ... & x_{B,N} & 1 & 0 & ... & 0 & 0\\
\\
0 & ... & 0 & 0 & x_{B+1,0} & ... & x_{B+1,N} & 1\\
\\
... & ... & ... & ... & ... & ... & ... & ...\\
\\
0 & ... & 0 & 0 & x_{M,0} & ... & x_{M,N} & 1\\
\end{pmatrix}\nonumber\\
\end{align}
where $B$ is the index of the breakpoint. We also perform PCA separately on each of the 
two chunks. By redesigning $\mathbf{X}$ in this way, we can fit for different PLD 
coefficients for each of the two chunks while preserving the covariance structure 
across the breakpoint. We also consider further subdivisions of the light curves into
three and four chunks, divided such that there are approximately equal number of 
data points in each of the chunks. As before, we perform the same bootstrapping technique
to identify the optimal number of chunks for each EPIC target.

\subsection{De-trending}
After optimizing the GP and choosing the order of the PLD, the number of principal components, and the 
number of light curve subdivisions, we de-trend each EPIC light curve using
Equations~\ref{eq:pcamodel} and \ref{eq:pcacoefficients}.

\section{Results}
\label{sec:results}
\subsection{Transit Injections}
\label{sec:injections}
Since the PLD basis vectors are obtained from the \emph{fractional} pixel fluxes,
they do not in principle contain any astrophysical signals. De-trending with PLD
should therefore preserve the transit shape and depth, a fact that is confirmed
for the transits of several hot Jupiters observed with \emph{Spitzer} \citep{DEM15}.
However, as we showed in \S\ref{sec:dmopt}, PLD can increase the
in-transit scatter when the transits are masked and too many principal components 
are present (see Figure~\ref{fig:overfitting}). Unmasked transits can also be affected
by overfitting, since PLD can exploit linear combinations of the white noise component
to decrease the transit depth and therefore improve the likelihood of the fit.

\begin{figure}[h]
  \begin{center}
      \psfig{file=images/injections.png,width=3.5in}
       \caption{Transit injection results. Each panel shows the fraction of transits
                recovered with a certain depth ratio $D/D_0$ (recovered depth divided
                by true depth). Blue histograms correspond to the actual injection
                and recovery process performed with our pipeline; red histograms
                correspond to transits injected directly into the \emph{K2SFF de-trended}
                light curves and are shown merely for comparison. The values to the left
                and right of each histogram are the median $D/D_0$ for our pipeline
                and for the \texttt{K2SFF} control run, respectively. The smaller values at the
                top indicate the fraction of transits recovered with depths lower and
                higher than the bounds of the plots. Finally, the two columns distinguish
                between runs where the transits were masked during PLD (left) and runs
                where they were not (right); the three rows correspond to different
                injected depths: $10^{-2}$, $10^{-3}$, and $10^{-4}$. PLD preserves
                transit depths if the transits are properly masked; otherwise, a small bias
                toward smaller depths is introduced.}
     \label{fig:injections}
  \end{center}
\end{figure}

In order to test whether our pipeline is robust against these effects, we ran a set
of transit injection and recovery tests on a sample of EPIC
targets that do not contain known transit events. We randomly selected 50 stars from
each magnitude bin in the range $8 \le K_p \le 18$ ($K_p$ is the Kepler bandpass magnitude)
for a total sample size of 500
stars per run. We then injected synthetic transits of varying depths into the
light curve by multiplying each pixel by a transit model generated by the 
\texttt{pysyzygy}\footnote{\texttt{\url{https://github.com/rodluger/pysyzygy}}}
package. For simplicity, we fixed the period at 3.57 days (for an average of about
20 transits per target) and the duration at 2.5 hours, assuming zero eccentricity
and quadratic limb darkening parameters $a = 0.45$ and $b = 0.30$. We then
ran our pipeline twice for each injection, once masking the transits when computing
the PLD coefficients, and once leaving them unmasked.

Performing a full transit search is outside the scope of this paper. Since our 
goal is to determine whether or not PLD can bias transit depths, we fix all the
parameters except for the depth at their true values and recover the transit
depth by linear regression, simultaneously fitting the baseline flux in the
vicinity of the transit with a third-order polynomial. Our results are shown
in Figure~\ref{fig:injections}.

Each panel shows two histograms of the recovered depth as a fraction of the true
depth, $D/D_0$. In blue, we plot the recovery results after de-trending with our pipeline.
As a control, we also injected transits directly into the \emph{de-trended} data from
the \texttt{K2SFF} pipeline. Columns correspond to transits that were masked (left) and
left unmasked (right) during the de-trending step. Rows correspond to different
injected fractional depths: $10^{-2}$, $10^{-3}$, and $10^{-4}$, ranging from a typical 
hot Jupiter to a roughly Earth-sized planet.

In the masked runs, we recover the transit depth with no bias; all three blue histograms
are Gaussian-shaped and have medians of 1.00, consistent with the control runs. Moreover, 
it is evident by inspection that the widths of the distributions are nearly identical
to the widths of those in the control runs. This is particularly important given the
concern that PLD could increase the in-transit scatter, which would lead to larger
uncertainty on the transit depth and a broader distribution. Since this is not the case,
we conclude that our optimization method (\S\ref{sec:dmopt}) is correctly preventing 
overfitting and not leading to bias or increased uncertainty on the transit depth.

In the unmasked runs, a small bias toward lower depth is visible from the histograms.
As we mentioned above, this is expected, since we have no term in our design
matrix (Equation~\ref{eq:designmatrix}) to explicitly model transits. Since the GP
is (by design) not flexible enough to capture transit events, the PLD basis vectors
pick up the slack and partially fit out the transit by inflating the white noise,
which slightly improves the quality of the fit. This is similar to the example in 
Figure~\ref{fig:sinusoid}, where PLD inflated the white noise to remove an astrophysical
signal. In fact, any de-trending technique that does not explicitly include a transit
model is vulnerable to this sort of overfitting. This is particularly true for deep
transits. Consider the example in Figure~\ref{fig:202072563}, which shows the (folded
and normalized) primary and secondary eclipses of the eclipsing binary EPIC 202072563. Three datasets are
presented: the raw SAP flux (left), the \texttt{K2SFF} flux (center), and
the \texttt{EVEREST} flux, obtained by masking the eclipses during the PLD step
(right). The \texttt{K2SFF} and \texttt{EVEREST} fluxes have been smoothed with a GP to 
remove astrophysical variability. Despite significantly reducing the noise, the
\texttt{K2SFF} pipeline decreased the depth of both eclipses by nearly 40\%. In contrast,
by masking the eclipses prior to the PLD step, our pipeline prevents them from becoming
shallower while successfully removing the instrumental noise. Note that had we not
masked the eclipse when solving for the PLD coefficients, our eclipses would be
similarly shallower.

\begin{figure}[h]
  \begin{center}
      \psfig{file=images/202072563.png,width=3.5in}
       \caption{Primary and secondary eclipses of EPIC 202072563, folded
                on a period of 2.1237 days. From left to right, the columns
                show the raw SAP flux, the \texttt{K2SFF}-de-trended flux, and the
                flux de-trended with our pipeline after masking the eclipses. Masking
                is critical to preserving the transit depth during de-trending.}
     \label{fig:202072563}
  \end{center}
\end{figure}

It is important to note that currently undetected transits are of course not masked
during our de-trending step, causing them to become slightly shallower (on the order
of 5\%) in the de-trended flux. If new transiting candidates are discovered with
\texttt{EVEREST} data, we recommend that the de-trending step be repeated either with
an explicit transit model or by masking the transit events. This will remove the bias
toward shallower transits.

It is also possible that transits with very low signal-to-noise may be completely
fit out during the de-trending step. As \cite{DFM15} point out, this is an inevitable
consequence of the de-trend-then-search method. It is \emph{always} best to use
a model that captures all the features of the data, allowing one to solve for 
instrumental noise, stellar variability, and transits \emph{simultaneously}. To this end
\cite{DFM15} explicitly include a transit model in their design matrix, solving
Equation~\ref{eq:pcacoefficients} over a fine grid of periods and transit epochs. This
eliminates the de-trending step in favor of a combined de-trending/planet
searching step, which effectively circumvents the overfitting problems inherent to
least-squares de-trending techniques. However, such an approach is very computationally
expensive, given that each light curve must be processed once for every combination
of transit parameters. We therefore defer this procedure to a future paper.

\subsection{Limitations}
\label{sec:limitations}
We find that there are two particular situations in which PLD is likely to fail: saturated stars
and crowded apertures. These limitations are inherent to the method itself and not
specific to \emph{K2} data.

\subsubsection{Saturated Stars}
\label{sec:saturated}
\begin{figure}[h]
  \begin{center}
      \psfig{file=images/saturation2.png,width=2.75in}
       \caption{Fractional pixel fluxes $p_{il} / \sum_k{p_{ik}}$
                for quarter 3 of Kepler-3, a $K_p = 9.2$ hot-Jupiter host
                observed by the original \emph{Kepler} mission. The panels
                are arranged according to the positions of the pixels on
                the detector, and the data is smoothed and folded on the orbital period
                of Kepler-3b. Saturated pixels are highlighted in red;
                overflow pixels are highlighted in blue. PLD fails for this
                system because the transit signal is present in several of
                the basis vectors.}
     \label{fig:saturation}
  \end{center}
\end{figure}
\begin{figure}[h]
  \begin{center}
      \psfig{file=images/201270464.png,width=3.5in}
       \caption{EPIC 201270464, a $K_p = 9.4$ saturated eclipsing binary. Plotted here
                is the raw flux (top), the \texttt{K2SFF} flux (center), and the \texttt{EVEREST}
                flux (bottom). PLD washes out the stellar variability along with most of the eclipses
                for some saturated stars.}
     \label{fig:201270464}
  \end{center}
\end{figure}
The \emph{Kepler} detectors begin to saturate for stars with $K_p \lesssim 11.5$,
leading to flux bleeding along the pixel columns \citep{GIL10}. Since the total
charge is well conserved down to about $K_p = 7$, this is not an issue for
aperture photometry; transit depths and shapes are preserved in the SAP flux.
However, the basic assumption of PLD---that the fractional pixel fluxes 
$p_{il} / \sum_k{p_{ik}}$ (see Equation~\ref{eq:pldmodel}) contain no astrophysical 
information---breaks down for these stars. This occurs because saturated pixels 
contain virtually no transit signal, as both the in-transit flux and the out-of-transit
flux are above the saturation level, resulting in a relatively featureless
timeseries. Since the total flux is conserved, the transit signal 
overflows into the adjacent pixels, traveling along the column until it reaches
an unsaturated pixel. In these ``overflow'' pixels, the fractional transit
depth is much larger than the true depth, since it contains the total transit signal
from all of the saturated pixels in that column. Consequently, the normalization
$p_{il} / \sum_k{p_{ik}}$ will only \emph{partially} remove the transit in the
``overflow'' pixels. Conversely, it will over-correct the saturated pixel fluxes,
leading to an inverted transit shape in the PLD basis vectors corresponding to 
those pixels. This is illustrated in Figure~\ref{fig:saturation}, which shows the 
\emph{fractional} pixel fluxes as a function of their position on the detector for the saturated
hot-Jupiter host Kepler-3b. We again choose a \emph{Kepler} target for illustrative
purposes, though the idea applies equally to \emph{K2}. Saturated pixels are indicated in red; ``overflow'' pixels
are indicated in blue; both groups of pixels contain the transit signal. Unsurprisingly, 
PLD fails to properly de-trend this target, removing most of the transit signal
along with the instrumental noise.

In Figure~\ref{fig:201270464}, we show an example of a saturated light curve of an eclipsing
binary processed with \texttt{K2SFF} and \texttt{EVEREST}. The \texttt{K2SFF} pipeline removes
a significant amount of the instrumental noise while preserving most of the astrophysical signal.
\texttt{EVEREST}, on the other hand, fits out both the stellar variability and the eclipses,
leading to spurious low scatter.

In principle, the saturation effects may be mitigated by collapsing each saturated column
into a single timeseries; since charge is conserved within individual columns, this would enforce
the correct transit depth in those ``superpixels,'' which could then be used as basis
vectors for PLD. However, we leave this procedure to future work. At present, we flag
saturated stars in our database that are at risk of PLD overfitting. As a general rule, 
while saturation may occur for stars as faint as $K_p = 11.5$, we find that PLD preserves transit
depths for most stars down to at least $K_p = 11$.

\subsubsection{Crowded Apertures}
\label{sec:crowded}
The other situation in which PLD performs poorly is in apertures containing significant
contamination from other stars. In simple aperture photometry, the transit depth
can be decreased in the presence of another star within the aperture. To correct for 
this, one can simply scale the depth by the (inverse of the) crowding metric, the ratio of 
the flux due to the planet host to the total flux in the aperture. However, this is not
the case for data de-trended with PLD, for reasons we describe below.

In the case of a single star, the pixel flux $p_{il}$ in the $l^\mathrm{th}$ pixel at the $i^\mathrm{th}$ time
is the product of the (position-dependent) stellar signal $a_{il}$ and the (position-independent) transit signal $\tau_{i}$,
such that the fractional pixel flux is
\begin{align}
\label{eq:crowdingonestar}
\frac{p_{il}}{\sum\limits_{k}p_{ik}} = \frac{a_{il}\tau_i}{\sum\limits_{k}a_{ik}\tau_i} = \frac{a_{il}}{\sum\limits_{k}a_{ik}},
\end{align}
which, as we have argued before, contains no transit signal. In the case of two stars, $a_{il}$ and $b_{il}$, 
the first of which contains a transit, we may write the fractional flux as
\begin{align}
\label{eq:crowdingtwostars}
\frac{p_{il}}{\sum\limits_{k}p_{ik}} = \frac{a_{il}\tau_i + b_{il}}{\sum\limits_{k}a_{ik}\tau_i + b_{ik}}.
\end{align}
Note that in this case the transit signal does not cancel, and the PLD basis vectors will contain some amount of
transit information, leading to possible overfitting as in the case of a saturated aperture. For a 
dim contaminant star, $b_{il} \ll a_{il}$, we may write
\begin{align}
\label{eq:crowdingtwostarsapprox}
\frac{p_{il}}{\sum\limits_{k}p_{ik}} \approx \frac{a_{il}}{\sum\limits_{k}a_{ik}}\left(1 + \frac{\Delta}{\tau_i}\right)
\end{align}
where\begin{align}
\label{eq:deltacrowding}
\Delta \equiv \left(\frac{b_{il}}{a_{il}} - \frac{\sum\limits_{k}b_{ik}}{\sum\limits_{k}a_{ik}}\right)
\end{align}
is a measure of the difference between the crowding in a given pixel and the crowding
in the entire aperture. In general, the larger the value of $\Delta$, the more power
PLD will have to fit out the transit signal. This is the case for \emph{bright contaminant
sources near the edge of the aperture}, for which the value of 
$b_{il}/a_{il}$ varies greatly across the aperture. Interestingly, for contaminant
sources co-located with the transiting planet host (as in the case of binary stars
or stars that are aligned but unresolved), the quantity $b_{il}/a_{il}$ is constant
across the detector and $\Delta = 0$, leading to a PLD basis set that does not contain
transit information. This can also be shown from the exact expression 
(Equation~\ref{eq:crowdingtwostars}). Crowding is therefore only a concern when
there is a bright contaminant sufficiently separated from the transiting planet host.
In practice, we find that PLD begins to overfit for contaminants that are 
separated by more than one pixel from the target and are either brighter than
or within $\sim 2$ magnitudes of the target.

It is possible to correct for the effects of crowding if the quantity $b_{il}/a_{il}$
is known, even if just approximately. However, this requires careful modeling of the
stellar PSF and is beyond the scope of this paper. In our database, we flag sources
that may suffer from overfitting due to crowded fields.

\subsection{Photometric Precision}
\label{sec:precision}
The formal photometric noise metric developed by the \emph{Kepler} team is the
Combined Differential Photometric Precision (CDPP) \citep{CHR12}, which is computed
by the \emph{Kepler} pipeline on transit-like timescales of 3, 6 and 12 hours.
The CDPP evaluated for a given duration is defined so that it is equivalent to the 
depth of a transit of that duration that would yield a SNR of 1. In this section we adopt a proxy 
of the 6-hr CDPP that is easier to calculate than
the formal metric defined in \cite{CHR12}. Our approach is very similar to the approaches
of \cite{GIL11}, \cite{VJ14} and \cite{AIG16}. In order to prevent correlated stellar
noise from inflating the white noise calculation, we apply a 2-day, quadratic Savitsky-Golay 
high-pass filter to the de-trended flux, clipping
outliers at 5$\sigma$. We then compute the standard deviation of all contiguous 13-cadence
chunks of data, take the median, and divide by $\sqrt{13}$ as in \cite{VJ14} to obtain
the approximate 6-hr photometric precision of the data, which we henceforth refer to
as the CDPP.

\label{sec:precision}
\begin{figure}[h]
  \begin{center}
      \psfig{file=images/precision1_savgol.png,width=3.5in}
       \caption{6-hr photometric precision as a function of Kepler magnitude $K_p$
                for all stars observed by \emph{Kepler} (yellow dots) and for
                all \emph{K2} targets in Campaigns 0-4 de-trended with \texttt{EVEREST}
                (blue). For most of the stars in the magnitude range $11 < K_p < 13.5$,
                our pipeline recovers the original \emph{Kepler} precision.}
     \label{fig:precision1}
  \end{center}
\end{figure}
\begin{figure}[h]
  \begin{center}
      \psfig{file=images/precision2_savgol.png,width=3.5in}
       \caption{A comparison of the raw \emph{K2} 6-hr precision (red), the \texttt{K2SFF}
       pipeline precision (black), the \texttt{EVEREST} precision (blue), and the
       approximate photon noise limit (yellow) as a function of $K_p$. Solid lines
       indicate the median in 0.5 magnitude-wide bins. Our pipeline yields better
       precision than \texttt{K2SFF} at all magnitudes $K_p > 11$, and approaches
       the photon limit for bright dwarf stars.}
     \label{fig:precision2}
  \end{center}
\end{figure}

In Figure~\ref{fig:precision1} we plot the CDPP of our de-trended fluxes for Campaign 0-4 
stars, excluding saturated targets (stars whose brightest pixel's median flux $> 1.6\times 10^5 \mathrm{e/s}$) 
and stars in highly crowded apertures (stars with $\Delta K_p < 5$ neighbors
inside the aperture or brighter neighbors within 2 pixels of the edge of the aperture),
since PLD is likely to lead to artificially low CDPP in those cases. \texttt{EVEREST}
CDPP values are plotted as blue dots. For comparison, in yellow we plot the CDPP calculated for the raw
SAP fluxes of all stars observed by the original \emph{Kepler} mission. 
\texttt{EVEREST} recovers the \emph{Kepler} CDPP for most $K_p \lesssim 13$ stars,
and yields light curves with CDPP within about 1.5 times that of \emph{Kepler}
for $13 \lesssim K_p \lesssim 15$. The sparser clump of stars above the bulk group between
$K_p = 11$ and 14 are likely giants, whose short-timescale pulsations are not efficiently
captured by the high-pass filter and thus appear to be more noisy; this clump is also
present in the \emph{Kepler} distribution.

In Figure~\ref{fig:precision2} we again plot the CDPP as a function of $K_p$, but this time
on a logarithmic scale, comparing the \texttt{EVEREST} values (blue dots) to the raw \emph{K2}
CDPP (red dots) and the approximate 6-hr photon limit (yellow dots), which we calculate as
\begin{align}
\label{eq:phot}
\mathrm{CDPP}_{phot} = \frac{10^6}{\sqrt{21600\times\bar{F}}}
\end{align}
where $\bar{F}$ is the average SAP flux for the target in $\mathrm{e/s}$. Solid lines indicate
the median CDPP in 0.5 magnitude-wide bins. \texttt{EVEREST} leads to nearly an order-of-magnitude
improvement in the CDPP for the brightest targets, approaching the photon limit for 
$K_p \lesssim 12$ dwarfs. Moreover, \texttt{EVEREST} yields lower average CDPP values than
\texttt{K2SFF} for all \emph{Kepler} magnitudes $\gtrsim 11$ (see the following section).

\subsection{Comparison to Other Pipelines}
\label{sec:comparison}
In this section we compare the precision of our de-trended light curves to that of the,
\texttt{K2VARCAT}, \texttt{K2SFF}, and \texttt{K2SC} light curves. Though other
pipelines exist \citep[e.g.,][]{LUN15, HUA15, CRO15}, here we focus on those that 
are also available for download at
the MAST HLSP \emph{K2} archive.\footnote{https://archive.stsci.edu/k2/hlsp/}
For each of the pipelines, we download all available de-trended light curves and
compute the CDPP as described in \S\ref{sec:precision}. In 
Figures~\ref{fig:comparison_k2varcat}-\ref{fig:comparison_k2sc}, we plot a pairwise
comparison of the precision achieved for all Campaign 0-4 light curves that are
available in both our catalog and that of the other pipeline. These plots are based
on Figure 10 in \cite{AIG16}; they show the relative 6-hr CDPP difference between
our light curves and those produced by the \texttt{K2VARCAT}, \texttt{K2SFF}, and \texttt{K2SC}
pipelines as a function of Kepler magnitude. Individual light curves are indicated by
blue dots, and the median in half magnitude-wide bins is shown as a black line.
As before, we trim
5$\sigma$ outliers and apply a high-pass filter prior to computing the CDPP.
We also exclude saturated stars and stars in highly crowded apertures.

Figure~\ref{fig:comparison_k2varcat} shows the comparison to the \texttt{K2VARCAT}
pipeline. Our de-trended precision is systematically better at all magnitudes shown;
on average, we achieve about double the precision (half the CDPP). There are very
few cases where \texttt{K2VARCAT} yields lower CDPP values than \texttt{EVEREST}
(points above zero).

Figure~\ref{fig:comparison_k2sff} shows the same plot, but comparing \texttt{EVEREST} to
\texttt{K2SFF}. Again, \texttt{EVEREST} yields higher median precision at all
magnitudes, by about 15\% for $K_p < 15$ and about 10\% for $K_p > 15$. In general, \texttt{EVEREST}
light curves also have significantly fewer outliers than \texttt{K2SFF} light curves.
This is clear from Figures~\ref{fig:detrended0} and \ref{fig:detrended1} in the
following section: \texttt{K2SFF} light curves often display a band of outliers
above the light curve continuum, most likely associated with thruster firings. PLD
naturally corrects for these, obviating the need to discard the several hundred thruster
firing data points in each campaign.

Finally, in Figure~\ref{fig:comparison_k2sc} we plot the comparison to the PDC version
of the \texttt{K2SC}
pipeline, whose performance is the best out of the three we consider here. In order to 
ensure the two datasets are compared on equal footing, we use the systematics-corrected
\texttt{K2SC} fluxes rather than the fully whitened fluxes; we obtain these by
summing the \texttt{FLUX} and \texttt{TREND\_T} columns in the dataset. We then apply
a Savitsky-Golay filter, as we do to the \texttt{EVEREST} data, to remove the stellar 
components of the variability.

\texttt{K2SC} yields slightly higher median precision than \texttt{EVEREST} (by about 5\%) for 
$15 \lesssim K_p \lesssim 17$. For $K_p \lesssim 14$ and $K_p \gtrsim 17$, \texttt{EVEREST}
light curves have higher precision, by 5-10\%. \texttt{K2SC} likely outperforms
\texttt{EVEREST} for fainter stars because the smaller PSF of faint
stars results in fewer, noisier basis vectors for PLD. The \texttt{K2SC} pipeline, on 
the other hand, uses the stellar motion information contained in the \texttt{POS\_CORR1}
and \texttt{POS\_CORR2} arrays of the MAST light curves, which are computed from the
motion of spacecraft as determined from the behavior of several bright stars in the field
of view; these metrics are subject to less noise and lead to slightly better precision
for $15 \lesssim K_p \lesssim 17$. Nevertheless, PLD actually yields higher precision 
for the faintest targets.

All plots shown here display a significant amount of scatter. While \texttt{EVEREST}
yields the lowest CDPP in most cases, we recommend comparing light curves from
the different pipelines when the highest precision is desired for specific targets.
Moreover, at this time \texttt{EVEREST} performs poorly for saturated
targets and for those in very crowded fields. Future versions of the pipeline will address
these issues.

\begin{figure}[h]
  \begin{center}
      \psfig{file=images/comparison_k2varcat_savgol.png,width=3.5in}
       \caption{Relative 6-hr CDPP difference between \texttt{EVEREST}
                and \texttt{K2VARCAT} light curves for Campaigns 0-4. Blue dots
                show differences for individual stars, while the black line
                indicates the median in 0.5 magnitude-wide bins. Negative
                values indicate higher precision in the \texttt{EVEREST}
                light curves; compare to Figure~10 in \cite{AIG16}. On average, \texttt{EVEREST} yields
                light curves with half the scatter for all Kepler magnitudes $K_p > 11$.}
     \label{fig:comparison_k2varcat}
  \end{center}
\end{figure}
\begin{figure}[h]
  \begin{center}
      \psfig{file=images/comparison_k2sff_savgol.png,width=3.5in}
       \caption{Same as Figure~\ref{fig:comparison_k2varcat}, but comparing 
                \texttt{EVEREST} to \texttt{K2SFF}. Our pipeline yields higher
                precision light curves for most \emph{K2} stars and does
                better on average for all Kepler magnitudes $K_p > 11$.}
     \label{fig:comparison_k2sff}
  \end{center}
\end{figure}
\begin{figure}[h]
  \begin{center}
      \psfig{file=images/comparison_k2sc_savgol.png,width=3.5in}
       \caption{Same as Figure~\ref{fig:comparison_k2varcat}, but comparing 
                \texttt{EVEREST} to \texttt{K2SC}. To ensure both sets of light curves
                are on the same footing, the \texttt{K2SC} CDPP
                is computed from the PDC flux corrected for the instrumental 
                systematics \emph{only}. As before, a Savitsky-Golay filter
                is then applied to both sets of light curves.}
     \label{fig:comparison_k2sc}
  \end{center}
\end{figure}

\subsubsection{Example Light Curves}
\begin{figure*}[t]
  \begin{center}
    \leavevmode
      \psfig{file=images/detrended0.png,width=6in}
       \caption{De-trended light curves for EPIC 201367065 \citep[K2-3,][]{CRO15}. \emph{Top}: The de-trended
                \texttt{K2SFF} flux (left) and the GP-smoothed flux folded on the period of 
                the super-Earth K2-3b (right). \emph{Bottom}: The de-trended \texttt{EVEREST}
                flux. The 6-hr CDPP is 30.9 ppm for \texttt{K2SFF} and 16.6 ppm for
                \texttt{EVEREST}, a factor of $\sim$ 2 improvement.}
     \label{fig:detrended0}
  \end{center}
\end{figure*}
\begin{figure*}[t]
  \begin{center}
    \leavevmode
      \psfig{file=images/detrended1.png,width=6in}
       \caption{De-trended light curves for EPIC 205071984, a Campaign 2 star with three
                known planet candidates \citep{SIN15}. As in Figure~\ref{fig:detrended0}, the
                \texttt{K2SFF} light curve and the folded transit of EPIC 205071984.01
                are shown at the top; the equivalent plots for \texttt{EVEREST} are shown
                at the bottom. The 6-hr CDPP is 56.1 ppm for \texttt{K2SFF} and 24.0 ppm for
                \texttt{EVEREST}, a factor of $\gtrsim$ 2 improvement.}
     \label{fig:detrended1}
  \end{center}
\end{figure*}
In Figures~\ref{fig:detrended0} and \ref{fig:detrended1} we plot our de-trended light curves
for two \emph{K2} planet candidate hosts, EPIC 201367065 and 205071984. These were chosen
specifically to illustrate the advantages of the PLD technique and are, in a sense, best
case scenarios. Both stars have $K_p \approx 12$ and are the only bright sources in their
apertures. (There are two faint sources, $K_p \approx 17$ and $K_p \approx 18$, 
at the edge of the aperture of EPIC 205071984, but they are faint enough as to not affect
the de-trending.) In each of the figures, we plot the \texttt{K2SFF} light curves at the top
and the \texttt{EVEREST} light curves at the bottom. To the right, we plot the folded
transits of one of their planet candidates after removing the stellar variability signal with a GP.

In both cases, the \texttt{EVEREST} precision is a factor of about 2 higher than that of
\texttt{K2SFF}: 30.9 ppm for EPIC 201367065 and 24.0 ppm for EPIC 205071984. This is visible 
in both the full light curve and in the folded transits,
which display significantly less scatter. Note, importantly, that the greater de-trending
power of \texttt{EVEREST} does not affect the low-frequency stellar variability, which is 
present equally in both sets of light curves.

The corresponding light curves in the \texttt{K2VARCAT} database have CDPP values of 43.1
and 63.4, respectively. At the time of writing, these light curves are not present in the
\texttt{K2SC} catalog.

\section{Conclusions}
\label{sec:conclusions}
In this paper we introduced \texttt{EVEREST}, a pipeline developed to yield the highest
precision light curves for \emph{K2} stars. \texttt{EVEREST} builds on the pixel-level
decorrelation (PLD) technique of \cite{DEM15}, extending it to third order in the pixel
fluxes and combining it with principal component analysis to yield a set of basis
vectors that together capture most of the instrumental variability in the data. A Gaussian
process (GP) is then used to remove the instrumental
systematics while preserving astrophysical signals. In order to prevent overfitting, we
develop a bootstrapping method to determine the ideal order of the PLD
model and the number of principal components to use in the fit, yielding reliable,
high precision de-trended light curves for all \emph{K2} campaigns to date.

We validate our model by performing transit injection and recovery tests and show that
when transits are properly masked, we recover the correct depth without any bias.
When transits are not masked (as in the case of undiscovered planets), PLD de-trending
can lead to slightly shallower transits by $\lesssim 5\%$. However, the increased
precision of the de-trended light curves should be sufficient to enable the
detection of previously unknown, small transiting planets around \emph{K2} \citep{KRU16}.

We further compare our de-trended light curves to those produced by other \emph{K2} pipelines.
\texttt{EVEREST} light curves have systematically higher precision than both \texttt{K2SFF}
and \texttt{K2VARCAT} for all Kepler magnitudes $K_p > 11$. \texttt{EVEREST} also outperforms
\texttt{K2SC} at all magnitudes except $15 \lesssim K_p \lesssim 17$, where \texttt{K2SC}
light curves have marginally higher precision on average. Currently, \texttt{EVEREST}
performs poorly for saturated targets and for those in highly crowded fields, though
these issues will be addressed in future versions of the pipeline.

Our catalog of de-trended light curves will be made publicly available at
\texttt{\note{\url{https://archive.stsci.edu/prepds/everest/}}} and will be constantly
updated for new \emph{K2} campaigns. The source code used to de-trend the light curves
is open source and available at \texttt{\url{https://github.com/rodluger/everest}}.

\bibliographystyle{apj}
\bibliography{k2}
\end{document}